{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Flatten\n",
    "from keras.layers import LSTM, Bidirectional, RepeatVector, Add, TimeDistributed, Reshape,Concatenate, Activation\n",
    "from keras.layers import Dense, Lambda\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "from keras.utils import plot_model\n",
    "from keras.models import model_from_json\n",
    "from keras import layers\n",
    "import tensorflow as tf\n",
    "from keras.callbacks import ModelCheckpoint,EarlyStopping, TensorBoard\n",
    "from keras.callbacks import Callback\n",
    "from keras.optimizers import Adam\n",
    "import os\n",
    "import time\n",
    "from keras.losses import kullback_leibler_divergence\n",
    "#from keras.utils import multi_gpu_model\n",
    "\n",
    "from my_classes import DataGenerator\n",
    "from data_prep import one_hot_decode, get_bars_dataset\n",
    "\n",
    "from sklearn.model_selection import ParameterGrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only for GPU use:\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure problem\n",
    "n_features = 131\n",
    "timesteps = 16\n",
    "\n",
    "learning_rate = 0.0001  # Learning rate.\n",
    "decay_rate = 0.9999  # Learning rate decay per minibatch.\n",
    "min_learning_rate = 0.00001 # Minimum learning rate.\n",
    "\n",
    "n_encoder_units = 512\n",
    "n_decoder_units = n_encoder_units\n",
    "latent_dim = 64\n",
    "\n",
    "dropout=0.3\n",
    "beta=0.2\n",
    "epochs = 20000\n",
    "batch_size = 64\n",
    "num_training_samples = 10000\n",
    "num_validation_samples = 1000\n",
    "steps_per_epoch = int(num_training_samples / batch_size)\n",
    "validation_steps = int(num_validation_samples / batch_size)\n",
    "\n",
    "cardinality = 131\n",
    "start_of_sequence = np.zeros(cardinality)\n",
    "start_of_sequence[cardinality-1] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# returns train, inference_encoder and inference_decoder models\n",
    "def define_models(n_encoder_units, n_decoder_units, latent_dim, dropout, epsilon_std):\n",
    "    \n",
    "    # define training encoder\n",
    "    encoder_inputs = Input(shape=(timesteps, n_features), name=\"encoder_inputs\")\n",
    "    \n",
    "    encoder0 = Bidirectional(LSTM(n_encoder_units, \n",
    "                                  dropout=dropout,\n",
    "                                  return_sequences=True, \n",
    "                                  unit_forget_bias=True,\n",
    "                                  name=\"bidirectional_encoder0\"))\n",
    "                             \n",
    "    encoder1 = Bidirectional(LSTM(n_encoder_units,\n",
    "                                  unit_forget_bias=True,\n",
    "                                  return_state=True,\n",
    "                                  name=\"bidirectional_encoder1\"))\n",
    "    \n",
    "    # intermediate outputs\n",
    "    encoder_im_outputs = encoder0(encoder_inputs)\n",
    "\n",
    "    # final outputs\n",
    "    encoder_outputs, forward_h, forward_c, backward_h, backward_c = encoder1(encoder_im_outputs)\n",
    "    \n",
    "    \n",
    "    # concatenating states\n",
    "    state = Add(name='add_states')([forward_h, forward_c, backward_h, backward_c])\n",
    "\n",
    "\n",
    "    # creating latent vectors\n",
    "    z_mean = Dense(latent_dim, \n",
    "                   name=\"z_mean\",\n",
    "                   kernel_initializer=tf.random_normal_initializer(stddev=0.001), \n",
    "                   bias_initializer='zeros')(state)\n",
    "    \n",
    "    z_log_var = Dense(latent_dim, \n",
    "                          name=\"z_log_var\",\n",
    "                          activation=tf.math.softplus,\n",
    "                          kernel_initializer=tf.random_normal_initializer(stddev=0.001), \n",
    "                          bias_initializer='zeros')(state)\n",
    "    \n",
    "    \n",
    "    # sampling layer\n",
    "    def sampling(args):\n",
    "        \"\"\"Sampling z from isotropic Gaussian\"\"\"\n",
    "        z_mean, z_log_var = args\n",
    "\n",
    "        eps = K.random_normal(shape=(K.shape(z_mean)[0], latent_dim), mean=0., stddev=epsilon_std)\n",
    "        return z_mean + K.exp(z_log_var)*eps\n",
    "\n",
    "    # sampling z\n",
    "    z = Lambda(sampling, name=\"z_sample\")([z_mean, z_log_var])\n",
    "\n",
    "        \n",
    "    # Initial states for decoder is from z\n",
    "    state_decoder_h = Dense(n_decoder_units, activation='tanh', name=\"state_decoder_h\")(z)\n",
    "    state_decoder_c = Dense(n_decoder_units, activation='tanh', name=\"state_decoder_c\")(z)\n",
    "\n",
    "    # Input to decoder lstm is concatenation of z and inputs\n",
    "    z_repeated = RepeatVector(timesteps, name=\"z_repeated\")(z)\n",
    "    decoder_inputs = Input(shape=(timesteps, n_features), name=\"input_layer_decoder\")\n",
    "    decoder_train_input = Concatenate(axis=2, name=\"decoder_train_input\")([decoder_inputs, z_repeated])\n",
    "\n",
    "    # training decoder\n",
    "    decoder_lstm0 = LSTM(n_decoder_units,\n",
    "                         unit_forget_bias=True,\n",
    "                         dropout=dropout,\n",
    "                         return_sequences=True,\n",
    "                         name=\"decoder_lstm0\")\n",
    "    \n",
    "    decoder_lstm1 = LSTM(n_decoder_units,\n",
    "                         unit_forget_bias=True,\n",
    "                         return_sequences=True,\n",
    "                         return_state=True,\n",
    "                         name=\"decoder_lstm1\")\n",
    "    \n",
    "    # intermediate outputs\n",
    "    decoder_im_outputs = decoder_lstm0(decoder_train_input, initial_state=[state_decoder_h, state_decoder_c])\n",
    "    decoder_outputs, _, _ = decoder_lstm1(decoder_im_outputs)\n",
    "\n",
    "    decoder_dense = TimeDistributed(Dense(n_features, activation='softmax'), name=\"decoder_dense\")\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "\n",
    "    # define inference encoder\n",
    "    encoder_model = Model(encoder_inputs, [state_decoder_h, state_decoder_c, z])\n",
    "    \n",
    "    \n",
    "    \n",
    "    # define inference decoder\n",
    "    decinf_input = Input(shape=(1, n_features+latent_dim), name=\"decinf_inputs\")\n",
    "\n",
    "    # defime input states\n",
    "    dec_input_state_c = Input(shape=(1, n_decoder_units), name=\"d_input_state_c\")\n",
    "    dec_input_state_h = Input(shape=(1, n_decoder_units), name=\"d_input_state_h\")\n",
    "    dec_input_states = [dec_input_state_h, dec_input_state_c]\n",
    "\n",
    "    # intermediate lstm outputs\n",
    "    decinf_im_outputs = decoder_lstm0(decinf_input,\n",
    "                                      initial_state=dec_input_states)\n",
    "    \n",
    "    # output is a vector of sequences, needs reshaping\n",
    "    decinf_im_outputs = Reshape((1,n_decoder_units))(decinf_im_outputs)\n",
    "    \n",
    "    # lstm outputs\n",
    "    decinf_outputs, state_h, state_c = decoder_lstm1(decinf_im_outputs)\n",
    "    decoder_states = [state_h, state_c]\n",
    "\n",
    "    # During inference the decoder output one element at the time\n",
    "    decoder_inference_dense = Dense(n_features, activation='softmax', name=\"decoder_inference_dense\")\n",
    "    decinf_outputs = decoder_inference_dense(decinf_outputs)\n",
    "    decoder_model = Model([decinf_input, dec_input_state_c, dec_input_state_h], [decinf_outputs] + decoder_states)\n",
    "\n",
    "\n",
    "    def vae_loss(encoder_inputs, decoder_outputs):\n",
    "        xent_loss = K.categorical_crossentropy(encoder_inputs, decoder_outputs)\n",
    "        kl_loss = beta * kullback_leibler_divergence(encoder_inputs, decoder_outputs)\n",
    "        loss = xent_loss + kl_loss\n",
    "        return loss\n",
    "    \n",
    "    optimizer = Adam(lr=learning_rate, amsgrad=True, clipnorm=1.0)\n",
    "    model.compile(optimizer=optimizer, loss=vae_loss, metrics=['acc'])     \n",
    "\n",
    "    return model, encoder_model, decoder_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "params = {'dim': (timesteps, n_features),\n",
    "          'batch_size': batch_size,\n",
    "          'shuffle': True}\n",
    "\n",
    "ID_list_dict = {}\n",
    "\n",
    "# Datasets\n",
    "ID_list = np.load(\"ID_lists/ID_list.npy\")\n",
    "\n",
    "ID_list_dict[\"train\"] = ID_list.item().get(\"train\")\n",
    "ID_list_dict[\"validation\"] = ID_list.item().get(\"validation\")\n",
    "\n",
    "# Generators\n",
    "training_generator = DataGenerator(ID_list_dict['train'], **params)\n",
    "validation_generator = DataGenerator(ID_list_dict['validation'], **params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and saving tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "train, _, _ = define_models(n_encoder_units=n_encoder_units,\n",
    "                                        n_decoder_units=n_decoder_units,\n",
    "                                        latent_dim=latent_dim,\n",
    "                                        dropout=dropout,\n",
    "                                        epsilon_std=1.)\n",
    "\n",
    "train.load_weights(\"weights/512_64/beta0.2-weights-improvement-441-0.99.hdf5\")\n",
    "\n",
    "tensorboard_directory = \"tb_trained_models/beta0.2\"\n",
    "\n",
    "tensorboard = TensorBoard(log_dir=tensorboard_directory, batch_size=batch_size)\n",
    "filepath=\"weights/512_64/beta0.2-weights-improvement-{epoch:02d}-{val_acc:.2f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [tensorboard,checkpoint]\n",
    "\n",
    "# Train model on dataset\n",
    "train.fit_generator(generator=training_generator,\n",
    "                    validation_data=validation_generator,\n",
    "                    steps_per_epoch=steps_per_epoch,\n",
    "                    validation_steps=validation_steps,\n",
    "                    use_multiprocessing=False,\n",
    "                    epochs=epochs,verbose=2,callbacks=callbacks_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
