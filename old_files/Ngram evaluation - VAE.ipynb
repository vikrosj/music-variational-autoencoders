{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from data_prep import one_hot_decode, get_bars_dataset, one_hot_encode, one_hot_decode_song\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from predictions import define_models,predict_sequence,predict_sequence_from_zs,predict_z_and_states\n",
    "from music21 import converter, instrument, note, chord, stream, midi, environment\n",
    "%matplotlib inline\n",
    "import os\n",
    "from midi_handling import noteArrayToStream, switch_range\n",
    "from music21 import *\n",
    "import numpy as np\n",
    "mdn_dataset_path = \"h5_files/mdn_dataset_b1.h5\"\n",
    "vae_weights_path = \"weights/512_64/weights-improvement-2455-1.00.hdf5\"\n",
    "import h5py\n",
    "from numba import jit\n",
    "from tqdm import tqdm\n",
    "\n",
    "from ngram_compute import int_list_to_str, compute_ngram_prob\n",
    "############################ MDN initalisation ###################################\n",
    "SEQ_LEN = 16\n",
    "BATCH_SIZE = 64\n",
    "HIDDEN_UNITS = 512\n",
    "EPOCHS = 400\n",
    "SEED = 2345  # set random seed for reproducibility\n",
    "\n",
    "\n",
    "OUTPUT_DIMENSION = 64\n",
    "NUMBER_MIXTURES = 10\n",
    "\n",
    "\n",
    "np.random.seed(SEED)\n",
    "############################ VAE initalisation ###################################\n",
    "n_features = 131\n",
    "timesteps = 16\n",
    "learning_rate = 0.0001\n",
    "n_encoder_units = 512\n",
    "n_decoder_units = n_encoder_units\n",
    "latent_dim = 64\n",
    "dropout=0.3\n",
    "beta=1\n",
    "############################ MUSESCORE intialisation ###############################\n",
    "environment.set(\"musescoreDirectPNGPath\", \"/usr/bin/musescore\")\n",
    "\n",
    "############################ NGRAM intialisation ###############################\n",
    "from decimal import *\n",
    "getcontext().prec = 6 # set max decimal points for ngram\n",
    "from math import log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separated in two parts:  \n",
    "VAE predictions  \n",
    "MDN predictions  \n",
    "  \n",
    "Evaluation is done with 500 16-bar songs from the dataset.  \n",
    "  \n",
    "Two real songs are chosen for each evaluation.  \n",
    "  \n",
    "For the VAE, each bar is predicted from a corresponding z.  \n",
    "  \n",
    "For the MDN, the first z is predicted from the VAE decoder, then the MDN predicts the rest of the song."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choose songs:\n",
    "\"\"\"\n",
    "song_count = 0\n",
    "pointer = 0\n",
    "\n",
    "with h5py.File(\"h5_files/As_and_Bs.h5\", \"w\") as hpyNgram:\n",
    "\n",
    "    while song_count < 1000:\n",
    "\n",
    "        check_song = np.empty((1,16,131))\n",
    "        songA = np.empty((1,16,131))\n",
    "        songB = np.empty((1,16,131))\n",
    "\n",
    "        song_from_fileA = np.load(\"songs_encoder_inputs/id-\" + str(pointer) + \".npy\")\n",
    "        song_lenA = song_from_fileA.shape[0]\n",
    "\n",
    "        song_from_fileB = np.load(\"songs_encoder_inputs/id-\" + str(pointer+1) + \".npy\")\n",
    "        song_lenB = song_from_fileB.shape[0]\n",
    "\n",
    "        if (song_lenA >= 10) and (song_lenB >=10):\n",
    "            print(song_count)\n",
    "            songA = song_from_fileA\n",
    "            songB = song_from_fileB\n",
    "\n",
    "            hpyNgram.create_dataset(\"A\" + str(pointer) + \"_z\" + str(pointer), data=song_from_fileA)\n",
    "            hpyNgram.create_dataset(\"B\" + str(pointer) + \"_z\" + str(pointer+1), data=song_from_fileB)\n",
    "            song_count +=1\n",
    "        else:\n",
    "            print(\"sometin\")\n",
    "\n",
    "        pointer += 1\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "train, infenc, infdec = define_models(n_encoder_units=n_encoder_units,\n",
    "                                        n_decoder_units=n_decoder_units,\n",
    "                                        latent_dim=latent_dim, \n",
    "                                      timesteps=timesteps,\n",
    "                                      n_features=n_features,\n",
    "                                      learning_rate=learning_rate,\n",
    "                                        dropout=dropout,\n",
    "                                      beta=beta,\n",
    "                                        epsilon_std=1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set VAE weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.load_weights(vae_weights_path)\n",
    "print(\"Loaded weights from disk\")\n",
    "\n",
    "all_weights = train.get_weights()\n",
    "decoder_position = len(infenc.get_weights())\n",
    "decoder_weights = all_weights[decoder_position::]\n",
    "infdec.set_weights(decoder_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize empty dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['a', 'c0', 'c1', 'c2', 'c3', 'c4', 'b']\n",
    "\n",
    "df_idx = 0\n",
    "df1 = pd.DataFrame(columns=cols, index=[df_idx])\n",
    "\n",
    "for i in range(1):\n",
    "    df1.loc[i].a = np.nan\n",
    "    df1.loc[i].c0 = np.nan\n",
    "    df1.loc[i].c1 = np.nan\n",
    "    df1.loc[i].c2 = np.nan\n",
    "    df1.loc[i].c3 = np.nan\n",
    "    df1.loc[i].c4 = np.nan\n",
    "    df1.loc[i].b = np.nan\n",
    "    \n",
    "with open('ngram_results.csv', 'w') as f:\n",
    "    df1.to_csv(f, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict intermediate songs and compute likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maximum likelihood for a sequence (with Laplace smoothing) is:\n",
    "\n",
    "```python\n",
    "MLE = (Count(n grams) + 1)/ (Count(n-1 grams) + V)\n",
    "```\n",
    "\n",
    "V is the number of unique tokens in the vocabulary, in this case: \n",
    "\n",
    "```python\n",
    "V = 130\n",
    "```\n",
    "\n",
    "The absolute worst MLE for a sequence is -759.3353742710694:\n",
    "\n",
    "```python\n",
    "for i in range(16 * 10 - 4):\n",
    "    prob += log(float(0 + 1 / 0 + 130))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = 5\n",
    "\n",
    "z_dataset = h5py.File(\"h5_files/z_dataset.h5\", \"r\")\n",
    "\n",
    "with h5py.File(\"h5_files/As_and_Bs.h5\", \"r\") as A_and_B:\n",
    "    \n",
    "    for item in tqdm(list(A_and_B.keys())):      \n",
    "\n",
    "        # update index for dataframe\n",
    "        df_idx += 1\n",
    "        \n",
    "        cols = ['a', 'c0', 'c1', 'c2', 'c3', 'c4', 'b']\n",
    "\n",
    "        # create temporary dataframe\n",
    "        df2 = pd.DataFrame(columns=cols, index=[df_idx])\n",
    "\n",
    "        \n",
    "        ################ song A ###################################\n",
    "        \n",
    "        # get song A compute ngram MLE\n",
    "        songA = np.array(A_and_B.get(item))[0:length]\n",
    "        A = one_hot_decode_song(songA)\n",
    "        \n",
    "        # add to dataframe position a\n",
    "        df2.loc[df_idx].a = compute_ngram_prob(A)\n",
    "\n",
    "        ################ song B ###################################\n",
    "        # get B position\n",
    "        endpos = int(item.split(\"_\")[0][1:]) + 1\n",
    "        B_pos = \"B\" + item.split(\"_\")[0][1:] + \"_z\" +  str( endpos )\n",
    "    \n",
    "        # get song B compute ngram MLE\n",
    "        B = one_hot_decode_song(np.array(A_and_B.get(B_pos))[0:length])\n",
    "        \n",
    "        # add to dataframe position b\n",
    "        df2.loc[df_idx].b = compute_ngram_prob(B)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        ################ latent z for interpolation ################\n",
    "        # get z-position for song \n",
    "        z_pos = item.split(\"_\")[1][1:]\n",
    "        \n",
    "        # get 5 first z's for songA\n",
    "        song_zA = np.array(z_dataset.get('z_list' + str(z_pos)))[0:length]\n",
    "        \n",
    "        # get 5 first z's for songB\n",
    "        song_zB = np.array(z_dataset.get('z_list' + str(int(z_pos)+1)))[0:length]\n",
    "\n",
    "        ################## interpolating ###########################\n",
    "        alpha = 0.1 \n",
    "        \n",
    "        # update alpha 9 times (0.1 - 0.9)\n",
    "        for i in range(5):\n",
    "            pred_song = []   \n",
    "            \n",
    "            # loop through 5 timesteps of a song\n",
    "            for j in range(length):\n",
    "                \n",
    "                z = alpha*song_zA[j] + float(1-alpha)*song_zB[j]\n",
    "\n",
    "                encoder_output = predict_sequence_from_zs(infdec,\n",
    "                                                 z,\n",
    "                                                 latent_dim,\n",
    "                                                 seq_len=timesteps,\n",
    "                                                 temperature=1,\n",
    "                                                 n_decoder_units=n_decoder_units)\n",
    "                \n",
    "                pred_song.append(one_hot_decode(encoder_output.reshape(timesteps, n_features)))\n",
    "\n",
    "            # compute ngram probability of song and add to dataframe row\n",
    "            c_pos = \"c\" + str(i)\n",
    "\n",
    "            df2.loc[[df_idx],[c_pos]] = compute_ngram_prob(np.ravel(pred_song))\n",
    "            \n",
    "            # update alpha\n",
    "            alpha += 0.2\n",
    "            \n",
    "        # append dataframe\n",
    "        with open('ngram_results.csv', 'a') as f:\n",
    "            df2.to_csv(f, header=False)\n",
    "\n",
    "\n",
    "z_dataset.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
