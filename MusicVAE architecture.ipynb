{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from numpy import array_equal\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Flatten\n",
    "from keras.layers import LSTM, Bidirectional, RepeatVector, Add, TimeDistributed, Reshape,Concatenate, Activation\n",
    "from keras.layers import Dense, Lambda\n",
    "from data_prep011018 import one_hot_decode, get_bars_dataset\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "from keras.utils import plot_model\n",
    "from keras.models import model_from_json\n",
    "from keras.losses import kullback_leibler_divergence\n",
    "import os\n",
    "from keras import layers\n",
    "import tensorflow as tf\n",
    "from my_classes import DataGenerator\n",
    "from keras.callbacks import ModelCheckpoint,EarlyStopping, TensorBoard\n",
    "from keras.callbacks import LearningRateScheduler, Callback\n",
    "from keras.optimizers import Adam, Adadelta\n",
    "from math import exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only for GPU use:\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure problem\n",
    "# configure problem\n",
    "n_features = 131\n",
    "timesteps = 16\n",
    "\n",
    "learning_rate = 0.0001  # Learning rate.\n",
    "decay_rate = 0.9999  # Learning rate decay per minibatch.\n",
    "min_learning_rate = 0.00001 # Minimum learning rate.\n",
    "\n",
    "n_encoder_units = 1024\n",
    "n_decoder_units = 1024\n",
    "latent_dim = 512\n",
    "\n",
    "dropout_rate=0.1\n",
    "\n",
    "epochs = 20000\n",
    "batch_size = 64\n",
    "num_training_samples = 10000\n",
    "num_validation_samples = 1000\n",
    "steps_per_epoch = int(num_training_samples / batch_size)\n",
    "validation_steps = int(num_validation_samples / batch_size)\n",
    "\n",
    "cardinality = 131\n",
    "start_of_sequence = np.zeros(cardinality)\n",
    "start_of_sequence[cardinality-1] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# returns train, inference_encoder and inference_decoder models\n",
    "def define_models(n_encoder_units, n_decoder_units, n_features, timesteps, latent_dim, epsilon_std):\n",
    "    \n",
    "    # define training encoder\n",
    "    encoder_inputs = Input(shape=(timesteps, n_features), name=\"encoder_inputs\")\n",
    "    \n",
    "    encoder0 = Bidirectional(LSTM(n_encoder_units, \n",
    "                                  dropout=dropout_rate,\n",
    "                                  return_sequences=True), \n",
    "                             \n",
    "                                  name=\"bidirectional_encoder0\")\n",
    "    encoder1 = Bidirectional(LSTM(n_encoder_units,\n",
    "                                  #dropout=dropout_rate,\n",
    "                                  return_state=True),\n",
    "                                  name=\"bidirectional_encoder1\")\n",
    "    \n",
    "    # intermediate outputs\n",
    "    encoder_im_outputs = encoder0(encoder_inputs)\n",
    "\n",
    "    # final outputs\n",
    "    encoder_outputs, forward_h, forward_c, backward_h, backward_c = encoder1(encoder_im_outputs)\n",
    "    \n",
    "    \n",
    "    # concatenating states\n",
    "    state = Add(name='add_states')([forward_h, forward_c, backward_h, backward_c])\n",
    "\n",
    "\n",
    "    # creating latent vectors\n",
    "    z_mean = Dense(latent_dim, \n",
    "                   name=\"z_mean\",\n",
    "                   kernel_initializer=tf.random_normal_initializer(stddev=0.001), \n",
    "                   bias_initializer='zeros')(state)\n",
    "    \n",
    "    z_log_var = Dense(latent_dim, \n",
    "                          name=\"z_log_var\",\n",
    "                          activation=tf.math.softplus,\n",
    "                          kernel_initializer=tf.random_normal_initializer(stddev=0.001), \n",
    "                          bias_initializer='zeros')(state)\n",
    "    \n",
    "    \n",
    "    # sampling layer\n",
    "    def sampling(args):\n",
    "        \"\"\"Sampling z from isotropic Gaussian\"\"\"\n",
    "        z_mean, z_log_var = args\n",
    "\n",
    "        eps = K.random_normal(shape=(K.shape(z_mean)[0], latent_dim), mean=0., stddev=epsilon_std)\n",
    "        return z_mean + K.exp(z_log_var)*eps\n",
    "\n",
    "    # sampling z\n",
    "    z = Lambda(sampling, name=\"z_sample\")([z_mean, z_log_var])\n",
    "\n",
    "        \n",
    "    # Initial states for decoder is from z\n",
    "    state_decoder_h = Dense(n_decoder_units, activation='tanh', name=\"state_decoder_h\")(z)\n",
    "    state_decoder_c = Dense(n_decoder_units, activation='tanh', name=\"state_decoder_c\")(z)\n",
    "\n",
    "    # Input to decoder lstm is concatenation of z and inputs\n",
    "    z_repeated = RepeatVector(timesteps, name=\"z_repeated\")(z)\n",
    "    decoder_inputs = Input(shape=(timesteps, n_features), name=\"input_layer_decoder\")\n",
    "    decoder_train_input = Concatenate(axis=2, name=\"decoder_train_input\")([decoder_inputs, z_repeated])\n",
    "\n",
    "    # training decoder\n",
    "    decoder_lstm0 = LSTM(n_decoder_units,\n",
    "                         #dropout=dropout_rate,\n",
    "                         return_sequences=True,\n",
    "                         name=\"decoder_lstm0\")\n",
    "    \n",
    "    decoder_lstm1 = LSTM(n_decoder_units,\n",
    "                         #dropout=dropout_rate,\n",
    "                         return_sequences=True,\n",
    "                         return_state=True,\n",
    "                         name=\"decoder_lstm1\")\n",
    "    \n",
    "    # intermediate outputs\n",
    "    decoder_im_outputs = decoder_lstm0(decoder_train_input, initial_state=[state_decoder_h, state_decoder_c])\n",
    "    decoder_outputs, _, _ = decoder_lstm1(decoder_im_outputs)\n",
    "\n",
    "    decoder_dense = TimeDistributed(Dense(n_features, activation='softmax'), name=\"decoder_dense\")\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "\n",
    "    # define inference encoder\n",
    "    encoder_model = Model(encoder_inputs, [state_decoder_h, state_decoder_c, z])\n",
    "    \n",
    "    \n",
    "    \n",
    "    # define inference decoder\n",
    "    decinf_input = Input(shape=(1, n_features+latent_dim), name=\"decinf_inputs\")\n",
    "\n",
    "    # defime input states\n",
    "    dec_input_state_c = Input(shape=(1, n_decoder_units), name=\"d_input_state_c\")\n",
    "    dec_input_state_h = Input(shape=(1, n_decoder_units), name=\"d_input_state_h\")\n",
    "    dec_input_states = [dec_input_state_h, dec_input_state_c]\n",
    "\n",
    "    # intermediate lstm outputs\n",
    "    decinf_im_outputs = decoder_lstm0(decinf_input,\n",
    "                                      initial_state=dec_input_states)\n",
    "    \n",
    "    # output is a vector of sequences, needs reshaping\n",
    "    decinf_im_outputs = Reshape((1,n_decoder_units))(decinf_im_outputs)\n",
    "    \n",
    "    # lstm outputs\n",
    "    decinf_outputs, state_h, state_c = decoder_lstm1(decinf_im_outputs)\n",
    "    decoder_states = [state_h, state_c]\n",
    "\n",
    "    # During inference the decoder output one element at the time\n",
    "    decoder_inference_dense = Dense(n_features, activation='softmax', name=\"decoder_inference_dense\")\n",
    "    decinf_outputs = decoder_inference_dense(decinf_outputs)\n",
    "    decoder_model = Model([decinf_input, dec_input_state_c, dec_input_state_h], [decinf_outputs] + decoder_states)\n",
    "\n",
    "\n",
    "    def vae_loss(encoder_inputs, decoder_outputs):\n",
    "        xent_loss = K.categorical_crossentropy(encoder_inputs, decoder_outputs)\n",
    "        kl_loss = 0.9*(- 0.5 * K.mean(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)))\n",
    "        loss = xent_loss + kl_loss\n",
    "        return loss\n",
    "\n",
    "\n",
    "    # decaying learning rate with minimum value\n",
    "    lr = learning_rate\n",
    "    \n",
    "    optimizer = Adam(lr=lr, amsgrad=True, decay=decay_rate)\n",
    "    model.compile(optimizer=optimizer, loss=vae_loss, metrics=['acc'])     \n",
    "\n",
    "    return model, encoder_model, decoder_model\n",
    "\n",
    "\n",
    "# define model\n",
    "train, infenc, infdec = define_models(n_encoder_units=n_encoder_units,\n",
    "                                        n_decoder_units=n_decoder_units,\n",
    "                                        n_features=n_features,\n",
    "                                        timesteps=timesteps,\n",
    "                                        latent_dim=latent_dim,\n",
    "                                        epsilon_std=1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "params = {'dim': (timesteps, cardinality),\n",
    "          'batch_size': batch_size,\n",
    "          'shuffle': True}\n",
    "\n",
    "ID_list_dict = {}\n",
    "\n",
    "# Datasets\n",
    "ID_list = np.load(\"ID_list.npy\")\n",
    "\n",
    "ID_list_dict[\"train\"] = ID_list.item().get(\"train\")\n",
    "ID_list_dict[\"validation\"] = ID_list.item().get(\"validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generators\n",
    "training_generator = DataGenerator(ID_list_dict['train'], **params)\n",
    "validation_generator = DataGenerator(ID_list_dict['validation'], **params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath=\"weights/weights.{epoch:02d}-{val_loss:.2f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "#earlystopping = EarlyStopping(monitor='val_loss', min_delta=1e-5, patience=60, verbose=1, mode='auto')\n",
    "tensorboard = TensorBoard(log_dir=\"./\", batch_size=batch_size)\n",
    "\n",
    "callbacks_list = [checkpoint,tensorboard]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"class lr_schedule(Callback):\n",
    "\n",
    "    def step_decay_schedule(self, initial_lr=learning_rate, decay_rate=decay_rate):\n",
    "        '''\n",
    "        Wrapper function to create a LearningRateScheduler with step decay schedule.\n",
    "        '''\n",
    "        def schedule(epoch):\n",
    "\n",
    "            lr = K.eval(self.model.optimizer.lr)\n",
    "\n",
    "            if lr <= minimum_learning_rate:\n",
    "                lr = minimum_learning_rate\n",
    "\n",
    "            else:\n",
    "                lr = initial_lr * decay_rate\n",
    "\n",
    "            print(\"Learning rate\", K.eval(self.model.optimizer.lr))\n",
    "        \n",
    "        return LearningRateScheduler(schedule)\n",
    "\n",
    "\n",
    "lrHistory = lr_schedule()\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load weights into new model\n",
    "train.load_weights(\"weights/weights.1379-3.40.hdf5\")\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model on dataset\n",
    "train.fit_generator(generator=training_generator,\n",
    "                    validation_data=validation_generator,\n",
    "                    steps_per_epoch=steps_per_epoch,\n",
    "                    validation_steps=validation_steps,\n",
    "                    use_multiprocessing=False,\n",
    "                    epochs=epochs,verbose=2,callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "max_seq_len=32,  # Maximum sequence length. Others will be truncated.\n",
    "z_size=32,  # Size of latent vector z.\n",
    "free_bits=0.0,  # Bits to exclude from KL loss per dimension.\n",
    "max_beta=1.0,  # Maximum KL cost weight, or cost if not annealing.\n",
    "beta_rate=0.0,  # Exponential rate at which to anneal KL cost.\n",
    "batch_size=512,  # Minibatch size.\n",
    "grad_clip=1.0,  # Gradient clipping. Recommend leaving at 1.0.\n",
    "clip_mode='global_norm',  # value or global_norm.\n",
    "# If clip_mode=global_norm and global_norm is greater than this value,\n",
    "# the gradient will be clipped to 0, effectively ignoring the step.\n",
    "grad_norm_clip_to_zero=10000,\n",
    "learning_rate=0.001,  # Learning rate.\n",
    "decay_rate=0.9999,  # Learning rate decay per minibatch.\n",
    "min_learning_rate=0.00001, # Minimum learning rate.\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "midivae",
   "language": "python",
   "name": "midivae"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
